#!/usr/bin/env python3
"""
RAG Retriever for Glamhair Multi Comparator
Production-ready wrapper for FAISS semantic search + Keyword fallback + Query Expansion + Typo Normalization

Author: Peppe
Date: 2026-01-22
Version: 1.4 - Typo Normalization Added
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

from src.config import (
    EMBEDDING_MODEL,
    TOP_K_PRODUCTS,
    BASE_DIR,
    DATA_DIR
)

logger = logging.getLogger(__name__)

# Paths
MODELS_DIR = BASE_DIR / 'models'

class ProductRetriever:
    """Semantic search retriever using FAISS + Keyword fallback + Query Expansion + Typo Normalization"""
    
    def __init__(self):
        self.index = None
        self.metadata = []
        self.model = None
        self.is_loaded = False
        
        self.index_file = DATA_DIR / 'embeddings' / 'faiss_index.bin'
        self.metadata_file = DATA_DIR / 'embeddings' / 'products_metadata.json'
        self.model_cache_dir = MODELS_DIR / 'embedding_model'
        
        # All products for keyword search
        self.all_products = []
        
        logger.info("ProductRetriever initialized")
    
    def load(self) -> bool:
        """Load FAISS index, metadata, and model"""
        try:
            logger.info("Loading FAISS index...")
            if not self.index_file.exists():
                logger.error(f"Index file not found: {self.index_file}")
                return False
            
            self.index = faiss.read_index(str(self.index_file))
            logger.info(f"✅ FAISS index loaded: {self.index.ntotal} vectors")
            
            logger.info("Loading metadata...")
            if not self.metadata_file.exists():
                logger.error(f"Metadata file not found: {self.metadata_file}")
                return False
            
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            
            logger.info(f"✅ Metadata loaded: {len(self.metadata)} products")
            
            if self.index.ntotal != len(self.metadata):
                logger.error(f"Mismatch: {self.index.ntotal} vs {len(self.metadata)}")
                return False
            
            # Load all products for keyword search
            products_file = DATA_DIR / 'products' / 'ALL_PRODUCTS_ENRICHED.json'
            if products_file.exists():
                with open(products_file, 'r', encoding='utf-8') as f:
                    self.all_products = json.load(f)
                logger.info(f"✅ All products loaded: {len(self.all_products)} for keyword search")
            
            logger.info(f"Loading embedding model: {EMBEDDING_MODEL}...")
            self.model = SentenceTransformer(
                EMBEDDING_MODEL,
                cache_folder=str(self.model_cache_dir)
            )
            logger.info(f"✅ Model loaded from cache")
            
            self.is_loaded = True
            logger.info("✅ ProductRetriever fully loaded and ready")
            return True
            
        except Exception as e:
            logger.error(f"Error loading retriever: {e}", exc_info=True)
            return False
    
    def normalize_query(self, query: str) -> str:
        """
        Normalize query to handle typos and variations
        
        Args:
            query: Original query
            
        Returns:
            Normalized query with typos corrected
        """
        query_lower = query.lower().strip()
        
        # Common typo corrections
        typo_corrections = {
            'sampoo': 'shampoo',
            'shampo': 'shampoo',
            'shampù': 'shampoo',
            'condicionatore': 'conditioner',
            'balsamo': 'conditioner',
            'maschera': 'mask',
            'trattamento': 'treatment',
            'siero': 'serum',
        }
        
        # Apply corrections word by word
        words = query_lower.split()
        corrected_words = []
        
        for word in words:
            corrected_word = typo_corrections.get(word, word)
            if corrected_word != word:
                logger.info(f"Typo corrected: '{word}' → '{corrected_word}'")
            corrected_words.append(corrected_word)
        
        return ' '.join(corrected_words)
    
    def expand_query(self, query: str) -> str:
        """
        Expand query with synonyms and related terms for better matching
        
        Args:
            query: Original search query
            
        Returns:
            Expanded query string with synonyms
        """
        # STEP 1: Normalize (fix typos) FIRST
        query = self.normalize_query(query)
        
        query_lower = query.lower()
        
        # Domain-specific expansions
        expansions = {
            'forfora': ['forfora', 'antiforfora', 'anti-forfora', 'anti forfora', 'dandruff', 
                       'purifying', 'purificante', 'desquamazione', 'scaglie', 'squame'],
            'prurito': ['prurito', 'irritazione', 'irritato', 'sensibile', 'scalp', 
                       'cute sensibile', 'lenitivo', 'calmante', 'soothing'],
            'cute': ['cute', 'cuoio capelluto', 'scalp', 'skin', 'testa'],
            'capelli grassi': ['grassi', 'grasso', 'sebo', 'oily', 'purificante', 
                              'equilibrante', 'normalizzante', 'sebum'],
            'capelli secchi': ['secchi', 'secco', 'idratante', 'nutriente', 'dry', 
                              'nutrimento', 'nutritivo', 'moisturizing'],
            'caduta': ['caduta', 'anticaduta', 'anti-caduta', 'anti caduta', 'rinforzante', 
                      'densificante', 'hair loss', 'loss prevention'],
            'crespo': ['crespo', 'anti-crespo', 'anticrespo', 'frizz', 'anti-frizz', 
                      'disciplinante', 'liscio', 'smooth'],
            'danneggiati': ['danneggiati', 'sfibrati', 'rovinati', 'riparazione', 'repair',
                          'ristrutturante', 'ricostituente'],
            'colorati': ['colorati', 'tinti', 'color', 'colore', 'protezione colore',
                        'color protection'],
        }
        
        expanded_terms = [query]
        
        # Check each expansion category
        for key, synonyms in expansions.items():
            if key in query_lower:
                expanded_terms.extend(synonyms)
                logger.info(f"Query expansion: '{key}' → added {len(synonyms)} synonyms")
        
        # Limit to avoid too long queries
        final_query = ' '.join(expanded_terms[:12])
        
        return final_query
    
    def keyword_search(self, query: str, top_k: int = 50) -> List[Dict]:
        """Keyword-based search fallback"""
        query_lower = query.lower()
        keywords = query_lower.split()
        
        matches = []
        for product in self.all_products:
            score = 0
            text_to_search = f"{product.get('nome', '')} {product.get('descrizione', '')} {product.get('brand', '')}".lower()
            
            # Exact phrase match (highest score)
            if query_lower in text_to_search:
                score += 10
            
            # Individual keywords
            for keyword in keywords:
                if len(keyword) > 2 and keyword in text_to_search:
                    score += 1
            
            if score > 0:
                product_copy = product.copy()
                product_copy['keyword_score'] = score
                matches.append(product_copy)
        
        # Sort by score and return top_k
        matches.sort(key=lambda x: x['keyword_score'], reverse=True)
        return matches[:top_k]
    
    def search(
        self,
        query: str,
        top_k: int = None,
        min_similarity: float = 0.2,
        price_max: float = None,
        categoria: str = None,
        brand: str = None,
        use_hybrid: bool = True
    ) -> List[Dict]:
        """
        Hybrid search: Typo Normalization + Query Expansion + Semantic + Keyword fallback
        
        Args:
            query: Search query
            top_k: Number of results to return
            min_similarity: Minimum similarity threshold
            price_max: Maximum price filter
            categoria: Category filter
            brand: Brand filter
            use_hybrid: If True, combines semantic and keyword search
            
        Returns:
            List of product dictionaries with similarity scores
        """
        if not self.is_loaded:
            logger.error("Retriever not loaded")
            return []
        
        if top_k is None:
            top_k = TOP_K_PRODUCTS
        
        try:
            # STEP 1: NORMALIZE (fix typos) + EXPAND QUERY
            expanded_query = self.expand_query(query)
            logger.info(f"Original query: '{query[:100]}'")
            if expanded_query != query:
                logger.info(f"Expanded query: '{expanded_query[:200]}'")
            
            # STEP 2: SEMANTIC SEARCH on expanded query
            query_embedding = self.model.encode(
                expanded_query,
                normalize_embeddings=True,
                convert_to_numpy=True
            )
            
            query_embedding = query_embedding.reshape(1, -1).astype('float32')
            
            search_k = top_k * 3 if any([price_max, categoria, brand]) else top_k * 2
            similarities, indices = self.index.search(query_embedding, search_k)
            
            semantic_results = []
            for similarity, idx in zip(similarities[0], indices[0]):
                if idx == -1:
                    continue
                
                if similarity < min_similarity:
                    continue
                
                metadata = self.metadata[idx].copy()
                metadata['similarity_score'] = float(similarity)
                metadata['source'] = 'semantic'
                
                if price_max and metadata['price'] > price_max:
                    continue
                
                if categoria and metadata['categoria'] != categoria:
                    continue
                
                if brand and metadata['brand'].lower() != brand.lower():
                    continue
                
                semantic_results.append(metadata)
            
            logger.info(f"Semantic search: {len(semantic_results)} results")
            
            # STEP 3: KEYWORD SEARCH (if hybrid enabled)
            if use_hybrid:
                logger.info("Running keyword search...")
                keyword_results = self.keyword_search(expanded_query, top_k=top_k*2)
                
                # Merge results, avoiding duplicates
                semantic_ids = {p['id'] for p in semantic_results}
                for kw_product in keyword_results:
                    if kw_product['id'] not in semantic_ids:
                        # Normalize keyword score to similarity scale
                        kw_product['similarity_score'] = min(0.5 + (kw_product['keyword_score'] * 0.05), 0.9)
                        kw_product['source'] = 'keyword'
                        semantic_results.append(kw_product)
                
                logger.info(f"After keyword merge: {len(semantic_results)} total results")
            
            # STEP 4: Sort by similarity score and return top_k
            semantic_results.sort(key=lambda x: x['similarity_score'], reverse=True)
            results = semantic_results[:top_k]
            
            logger.info(f"✅ Final results: {len(results)} products")
            
            # Log top 3 for debugging
            if results:
                logger.info("Top 3 results:")
                for i, product in enumerate(results[:3], 1):
                    logger.info(f"  {i}. {product['nome'][:50]} (score: {product['similarity_score']:.3f}, source: {product['source']})")
            
            return results
            
        except Exception as e:
            logger.error(f"Search error: {e}", exc_info=True)
            return []
    
    def get_stats(self) -> Dict:
        """Get retriever statistics"""
        if not self.is_loaded:
            return {'loaded': False}
        
        return {
            'loaded': True,
            'total_products': len(self.metadata),
            'all_products': len(self.all_products),
            'index_size': self.index.ntotal,
            'model': EMBEDDING_MODEL,
        }

# Global instance (singleton pattern)
_retriever_instance = None

def get_retriever() -> ProductRetriever:
    """Get or create retriever instance"""
    global _retriever_instance
    
    if _retriever_instance is None:
        logger.info("Creating new retriever instance...")
        _retriever_instance = ProductRetriever()
        if not _retriever_instance.load():
            logger.error("Failed to load retriever")
            return None
    
    return _retriever_instance

def format_products_for_context(products: List[Dict], max_products: int = 30) -> str:
    """
    Format product list for Claude context with enhanced details
    """
    if not products:
        return "Nessun prodotto trovato per questa ricerca."
    
    products = products[:max_products]
    
    formatted_products = []
    for i, product in enumerate(products, 1):
        # Base product info
        product_lines = [
            f"{i}. {product['nome']}",
            f"   ID: {product['id']}",
            f"   Brand: {product['brand']}",
            f"   Categoria: {product['categoria']}",
            f"   Prezzo: €{product.get('price', 0.0):.2f}",
        ]
        
        # Add description if available
        if 'descrizione' in product and product['descrizione']:
            desc = product['descrizione'][:200]
            if len(product.get('descrizione', '')) > 200:
                desc += "..."
            product_lines.append(f"   Descrizione: {desc}")
        
        # Add image URL if available
        if 'immagine' in product and product['immagine']:
            product_lines.append(f"   Immagine: {product['immagine']}")
        
        # Add ingredients if available
        if 'ingredienti' in product and product['ingredienti']:
            ingredients = product['ingredienti'][:150]
            if len(product.get('ingredienti', '')) > 150:
                ingredients += "..."
            product_lines.append(f"   Ingredienti chiave: {ingredients}")
        
        # Add usage instructions if available
        if 'modo_uso' in product and product['modo_uso']:
            usage = product['modo_uso'][:100]
            if len(product.get('modo_uso', '')) > 100:
                usage += "..."
            product_lines.append(f"   Utilizzo: {usage}")
        
        # Always add URL and relevance score
        product_lines.append(f"   URL: {product['url']}")
        product_lines.append(f"   Rilevanza: {product.get('similarity_score', 0):.2f} ({product.get('source', 'unknown')})")
        
        formatted_products.append("\n".join(product_lines))
    
    header = f"PRODOTTI DISPONIBILI ({len(products)} risultati):\n\n"
    return header + "\n\n".join(formatted_products)